IEEE SENSORS JOURNAL, VOL. 19, NO. 23, DECEMBER 1, 2019 11475
A Robust 2D-SLAM Technology With
Environmental Variation Adaptability
Li-Hsin Chen and Chao-Chung Peng
Abstract — Simultaneous localization and mapping (SLAM) in
complicated indoor/outdoor unknown environments is challeng-ing. With a demand on high mobility and high integrity intelligentrobotics, it is desired that the SLAM system should be portableand possibly standalone. To carry out the pose estimation aswell as the mapping without relying on the information fromother sensors, such as image, inertial measurement unit, rotaryencoder of ground vehicle and so on, a single 2D light detectionand ranging (LiDAR) is considered in this paper. In order tofulﬁll a robust 2D SLAM technology in unknown environments,the principal component analysis (PCA) is utilized to evaluateLiDAR scan contours and to carry out a corridor detector.The corridor detector is further extended to achieve adaptiveunstable points removal, mapping probability adjustment as wellas loop closure. Based on an adaptive grid map segmentationscheme, the cumulative mapping errors can obviously be reducedand a precise 2D map can be eventually carried out. Manyexperiments are conducted to verify the proposed method.Finally, for comparison, this paper utilizes the scan data andground truth provided by the Computer Science and ArtiﬁcialIntelligence Laboratory (CSAIL) at the Massachusetts Instituteof Technology (MIT), to verify the localization precision of theproposed algorithm. Experiment shows that from the scan datain the route up to about 350 m, the maximum error can be aslow as about 20 cm.
Index Terms — Simultaneous localization and mapping
(SLAM); light detection and ranging (LiDAR); iterative closest
point (ICP); occupancy grid map; loop closure.
I. I NTRODUCTION
WE are entering an era where vehicles where vehicles
and robots need to be equipped with cognitive capa-
bility so that they will be able to explore the external envi-
ronment and operate autonomously. Simultaneous localization
and mapping (SLAM) is the core technology responsible formaking this the implementation possible. SLAM uses sensors
to carry out a spatial scan, obtain the geometric outline of
the environment and build a map for the robot to explore
the environment. Furthermore, the sensor localization can be
carried out based on the built map, and the map can beupdated according to the localiza tion results and the scan data
from the sensor. Accordingly, the spatial cognition capability
Manuscript received May 31, 2019; revised July 11, 2019; accepted
July 11, 2019. Date of publication July 26, 2019; date of current versionNovember 13, 2019. This work was supported by the Ministry of Science andTechnology under Grant MOST 107-2221-E-006-114-MY3 and Grant MOST
108-2923-E-006-005-MY3. The associate editor coordinating the review of
this article and approving it for publication was Dr. Ashish Pandharipande.(Corresponding author: Chao-Chung Peng.)
The authors are with the Departmen t of Aeronautics and Astronautics,
National Cheng Kung University, T ainan 701, Taiwan (e-mail: ccpeng@
mail.ncku.edu.tw).
Digital Object Identiﬁer 10.1109/JSEN.2019.2931368is signiﬁcantly important in environmental cognition. Hence,
the ranging sensor is most suitable for the implementation. Par-
ticularly, LiDAR has a higher accuracy in measuring distances,which is much more beneﬁcial fo r precision map building and
self-localization.
In recent years, 2D SLAM relevant technologies have been
widely applied in autonomous ground vehicles in the pursuit
of smart factories. Based on the applications, many issues
are raised and discussed acco rdingly. As mentioned by the
outstanding SLAM review paper [1], for many applications
and environments, numerous major challenges and importantquestions remain unsolved. One of the most important issues
is the adaptability of the 2D SLAM for the environment vari-
ations. As a result, it is worthy to pursue a robust perceptionand navigation system.
Since 1987, ranging sensors such as Sound Navigation And
Ranging (Sonar) and LiDAR have been developed as vehicle
sensors, and the occupancy grid mapping approach has been
utilized to build obstacle mapping for navigation [2], [3].In 1991, the extended Kalman ﬁlter (EKF) was implemented
in the sonar’s measurement model and vehicle’s model for the
ﬁrst time to locate the vehicle in a known environment [4].After that, the EKF was utilized to estimate the location of
landmark and the pose of the vehicle, and at the same time
the vehicle localization and the landmark map building are
carried out, which can be referred to as EKF-SLAM [5]–[11].
During the development of an occupancy grid map,
the binary Bayes ﬁlter was ﬁrst used in order to update the
occupancy grid map [2], [3]; then, the map updating approach
is simpliﬁed by using the binary Bayes ﬁlter with the oddsratio of occupied and unoccupied grids [12]. Based on the
above research, the mechanism of updating the grid map with
the scanning characteristics of the sensor can be referred to
as “inverse sensor model”, and the approach of updating the
grid map by using the “forward model” is also proposed [13].The occupancy grid map is widely used on the robot
systems [14]–[20].
However, most of the SLAM algorithms rely on the infor-
mation from the rotary encoder of ground vehicle, hence the
localization and map building cannot be carried out indepen-
dently. In addition, signiﬁcant cumulative errors occur due to
the long-term operation of SLAM. A large-scale (to suburban
area) SLAM, as well as the SLAM and Moving ObjectsTracking (SLAMMOT) were carried out in 2007 [21]. The
implementation actively redu ces the impact of moving objects
on a static map. Also, in the entire architecture, the regionalsubmaps are built, and the pose of the vehicle in the global
1558-1748 © 2019 I EEE. Personal u se is perm itted, but republication/redistri bution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on July 15,2022 at 16:19:25 UTC from IEEE Xplore.  Restrictions apply. 11476 IEEE SENSORS JOURNAL, VOL. 19, NO. 23, DECEMBER 1, 2019
map can be obtained by the loop closure from submaps to the
global map. In [22], a large-scale outdoor SLAM is carried out
with the data association of outdoor scan, and the approach
of calculating histogram correlation is used to determine theloop closure.
As a well-known SLAM algorithm, GMapping [23] is
based on Rao-Blackwellized particle ﬁlters relied on wheelodometry. Different from GMapping, Hector SLAM [24] and
Cartographer [25] use an odometry denied scan matcher,
which solves the optimal pose to maximize the occupancy
probability of the laser scan on grid map. Moreover, Car-
tographer deals with the loop closure by applying SparsePose Adjustment [26] to solve the pose graph optimization.
Afterward, comparisons and evaluations among these three
algorithms are made by [27], [28].
Recently, various methods and approaches in SLAM have
been presented. The study [29] analyzes the contour, or shape,
of LiDAR scan to approach a LiDAR odometry. The paper [30]
proposes a forward compensation of pose estimation by apply-
ing fast Fourier transform to LiDAR scan image. In [31], cor-rentropy, similar to Gaussian weight, is introduced to compute
rigid registration of scan matching with high precision and
robustness. In [32], the pose graph optimization is speeded upby sampling the usable pose nodes according to the designed
score. Additionally, several researches [33]–[35] intend to
extract line feature from LiDAR scan for line-based mapping
and localization-aiding. In [36], line features of LiDAR scan is
used to form its orientation histogram for absolute orientationsensing (relative to the building North) and loop closure.
Another feature extraction algorithm [37] uses corner as well
as line to recognize proper correspondences so as to enhancelocalization speed. However, line feature extraction might be
unsuitable for some complex environments.
Robots and unmanned vehicles can be implemented in a
wider range of applications, to fulﬁll the highly commer-
cialized demand for them. In view of this, the robustnessand usability of the SLAM algorithm will become signif-
icantly important when facing a more varied environment.
Therefore, the research goal of this study is to develop atwo-dimensional LiDAR algorithm for localization and map-
ping, so that the localization and operation can be carried out
independently without relying on other sensors. Moreover, the
loop closure will be utilized for the error correction, to create a
comprehensive and low-cost localization and mapping system.
II. P
OSE ESTIMATION
On the basis of LiDAR, the pose estimation is to match and
align the scan points with obstacle points in the map, and then
go back to calculate the pose of LiDAR in the map, to achievethe purpose of pose estimation.
Let
Bpi(k)denote the returned kthscan point from the ith
scan in the 2-D LiDAR sensor coordinate system, and let
Eqi(k)denote the obstacle point in the mapping coordinate
system matching toBpi(k), then the geometric relationship
between them is as follows :
Eq(k)
i=R(k)Bp(k)
i+t(k)(1)where R(k)represents the sensor’s rotation matrix of yawing
angle of the kthscan in the map, and t(k)represents the sensor’s
translation vector in the map. The LiDAR pose estimation
resolves the following question :
/parenleftBig
ˆR(k),ˆt(k)/parenrightBig
=arg min
R,t/summationdisplay N
iwi/vextenddouble/vextenddouble/vextenddouble/parenleftBig
RBp(k)
i+t/parenrightBig
−Eq(k)
i/vextenddouble/vextenddouble/vextenddouble2
subject to RTR=I (2)
where ˆR(k)andˆt(k)is the pose estimation for the kthscan,
andwiis the weight factor of each scan point in cost
function. Selection of the weight factor will be addressed inthe following section. To enhance the computation efﬁciency
for 2D SLAM case, an analytic solution is derived without
the use of a singular value decomposition (SVD). Please referto Appendix A for its analytical solution. However,
Eqi(k)is
also an unknown parameter in implementation. To deal with
that, ICP, which is an algorithm using the iterative approach
to searchEqi(k), is introduced.
Iterative Closest Point (ICP) [38] is a point-alignment
algorithm with simpler structure, and it is able to calculate the
rotation and translation betw een two sets of unmatched points.
Its algorithm disassembles (2) into the following recursion :
/parenleftBig
/lscript/Delta1R,/lscript/Delta1t/parenrightBig
=arg min
R,t‘/summationdisplay N
iwi/vextenddouble/vextenddouble/vextenddouble/parenleftBig
R/lscriptpi+t/parenrightBig
−/lscriptqi/vextenddouble/vextenddouble/vextenddouble2
subject to RTR=I (3)
/lscriptqi=arg min
qj∈Q||/lscriptpi−qj|| (4)
⎧
⎪⎨
⎪⎩/lscript+1R=/lscript/Delta1R/lscriptR
/lscript+1t=/lscript/Delta1R/lscriptt+/lscript/Delta1t
/lscript+1pi=/lscript/Delta1R/lscriptpi+/lscript/Delta1t(5)
where the left superscript of the variables represent the loop
index of the ICP. In the /lscriptthiteration, the closest point to/lscriptpi
is regarded as the corresponding matching point/lscriptqiin (4),
and it will be taken to (3) for rigid alignment to calculate
the pose increment,/lscript/Delta1Rand/lscript/Delta1t. After that,/lscriptR,/lscripttand the
scan points/lscriptpican be updated through (5) to search for the
new matching point/lscript+1qi. Follow the repeating procedures to
update the points continuously, the correct matching point canthen be found. When iteration converges,
/lscriptRand/lscripttwill be
the desired pose estimation, ˆR(k)andˆt(k).
However, if ICP is utilized to carry out the LiDAR pose
estimation by aligning the scan points with obstacle points in
the map, some details and process should be taken into con-
sideration, including initial pose compensation, outlier removal
and weight design in rigid alignment. Therefore, the overall
LiDAR estimation process based on ICP is as shown in Fig. 1,and the details will be described in the subsequent section.
A. Initial Pose Compensation
The procedure of “Closest Point Matching” in ICP will
likely search the wrong matching points, especially when the
initial poses between two sets of points have a signiﬁcantdifference. It will make the solution converge to the local
optimum, and the points set will converge to the wrong pose,
as shown in Fig. 2. Therefore, initial pose compensation for
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on July 15,2022 at 16:19:25 UTC from IEEE Xplore.  Restrictions apply. CHEN AND PENG: ROBUST 2D-SLAM TECHNOLOGY WITH ENVIRONMENTAL V ARIATION ADAPTABILITY 11477
Fig. 1. The ﬂowchart of LiDAR pose estimation by using the ICP.
Fig. 2. The points will converge to the wrong pose if the initial poses between
points have a signiﬁcant difference.
the points in ICP (i.e.,0/Delta1Rand0/Delta1t)is very important.
In order to reduce the pose difference between initial points
0pi(i.e., the scan dataBpi(k))and target matching points
Eqi(k)in ICP when solving ˆR(k)andˆt(k), the sensor pose
at the kthscan, the pose estimation ˆR(k−1)andˆt(k−1)from
the last scan can be used as the initial pose compensation,
0/Delta1Rand0/Delta1t, for the current ICP.
B. Outlier Removal
The closest point matching in ICP also includes the wrong
matching points for the calculation of rigid alignment, whichresults in pose miscalculation for the points, as shown in Fig. 3.
Therefore, when processing the closest point matching in ICP,
the matched outlier should be discarded. The removal can bedetermined by the distance between the matching points. The
matched point that is too far apart will be regarded as the
matched outlier. It will be discarded and will not be included
in the calculation of rigid alignment, which is :
Discard the matching pair (
/lscriptpi,/lscriptqi)
if/bardbl/lscriptpi−/lscriptqi/bardbl>dTHD (6)
where dTHD is the distance threshold of matching points, and
its value can be determined by the scan frequency and themoving speed of the sensor. Under scan frequency of 40 Hz
and sensor moving speed of 3 m/sec without sharp rotation,
the sensor displacement will be about 75 mm, and the distanceFig. 3. The closest point matching of outlier will cause pose estimation error
in ICP: (a) Matching of initial points. (b) Iterative convergence is inﬂuenced
by the closest point matching of outlier.
Fig. 4. The ICP results after the removal of the matched outlier.
between the corresponding matching points in LiDAR scan
will not exceed this limit. Hence, dTHD can be set to 100 mm
to 200 mm, and the matched outlier can be discarded, as shown
in Fig. 4.
C. Weight Design in Rigid Alignment
In the weight design of ICP rigid alignment in (3), the distri-
bution density of scan pointsBpi(k)is inversely proportional
to the scan distance ri(k)due to the radial scan of LiDAR.
For example, the surface of an object far from LiDAR will be
sketched by more scan points than the one closer to LiDAR,
as shown in Fig. 5. In order to balance this situation, the weight
wiin (3) is designed to be :
Rwi=/braceleftBigg
ri,ri>¯r
0,otherwise,
where
¯r=1
N/summationdisplay N
iri (7)
The distribution of the distance weight (7) is determined
by the scan distance ri, to compensate for the distribution
difference of scan point density at different scan distances, andto balance the inverse relationship between the scan density
and the scan distance with the r adial scan. In addition, (7) sets
the weight of scan points that are close to the average scan
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on July 15,2022 at 16:19:25 UTC from IEEE Xplore.  Restrictions apply. 11478 IEEE SENSORS JOURNAL, VOL. 19, NO. 23, DECEMBER 1, 2019
Fig. 5. Distribution density of scan points of LiDAR is inﬂuenced by the
san distance.
distance ¯rto zero, to reduce the impact from the scan points in
short distance on the pose estimation. Hence, the scan points in
long distance can align to their matching points in the map, and
the accuracy of the sensor’s pos e estimation will be enhanced.
Similarly, the inverse weight can be designed as the following :
Iwi=/braceleftBigg
1/rmin−1/ri,1/ri<
1/r
0, otherwise
where
1/r=1
N/summationdisplay N
i1
ri(8)
In comparison with (7), the weight distribution of inverse
weight III is more concentrated on the scan points far from
LiDAR, so that the sensor pose estimation is able to overcome
sharper rotation and movement.
III. M APCONSTRUCTION
In SLAM, map construction is an important part. The built
map can be used as a system output, as well as a feedback
for the localization of pose estimation algorithm, to enhance
accuracy and robustness of the localization.
A. Occupancy Grid Map
The coordinates of scan points obtained by LiDAR can be
conﬁrmed after the localization. The coordinates indicate theposition of obstacles, which represents the obstacle map as
shown in Fig. 6 (a). The occupancy grid map can divide the
continuous spaces into ﬁnite grids, to compute the countlessscan points and determines whether the grid is occupied by
an obstacle, as shown in Fig. 6 (b).
B. Binary Bayes Filter
The binary Bayes ﬁlter i s utilized to update the occu-
pancy grid map based on the derivation of probability the-ory [12], [39]. The set of observation of LiDAR from the
1
stscan to the kthscan), denoted by z(1:k),w a st a k e ni n t o
consideration (the superscript colon of z“:” represents “con-
tinuous intersection”) to calculate the probability p(m/lscript|z(1:k))
of the /lscriptthgrid point m/lscriptthat is occupied by the obstacle in the
map. The probability is indicated by the color of the grids.
p(m/lscript|z(1:k))=0 for white grids, and p(m/lscript|z(1:k))=1f o r
black ones, as shown in Fig. 7 (a).Fig. 6. LiDAR scan points and mapping: (a) The scan points of obstacle
will overlap and accumulate over the scan time. (b) The occupancy grid map
can divide spaces into grids and compute the countless scan points with ﬁnitegrids, to complete the map construction.
Fig. 7. The occupancy grid map and the extracted scan points on obstaclemap: (a) The color of grid map indicates the probability of grid in occupancy.(b) The grids with high probability will be extracted to be the obstacle pointsin the map.
In sensor pose estimation, the black grids will be extracted
from the grid map to be the obstacle points in the map,
and they will be aligned with the scan points of the sensor,
as shown in Fig. 7 (b). In this approach of extracting obstacle
points, the density of points will be fully depended on theresolution of grid in the map. In addition, the obstacle points
need to be matched and aligned with the scan points in the
pose estimation. According to the alignment strategy in (7),the focus of the alignment of points will be at the scan points
in long distance. Thus, the resolution of the grid map must
not be less than those scan points so that the grid map is able
to provide sufﬁcient and detailed geometric information of the
obstacle. For example, the longest scan distance of LiDAR willbe limited to about 5 m in an ordinary indoor space. As for the
LiDAR with 0.25 degrees of angular resolution, the spacing
between the scan points is about 20 mm in a scan distanceof 5 m, so it is more appropriate to set the grid size to 20 mm
or smaller. If the longest scan distance of LiDAR at a broader
space is kept to about 10 m, then the interval of scan points
will be wider; hence, the grid resolution of the map can be
relaxed to about 50 mm based on the same calculation.
The updated formula for the pr obability of the binary Bayes
ﬁlter would be :
p(m
/lscript|z(1:k))
1−p(m/lscript|z(1:k))
=p(m/lscript|z(k))
1−p(m/lscript|z(k))p(m/lscript|z(1:k−1))
1−p(m/lscript|z(1:k−1))1−p(m/lscript)
p(m/lscript)(9)
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on July 15,2022 at 16:19:25 UTC from IEEE Xplore.  Restrictions apply. CHEN AND PENG: ROBUST 2D-SLAM TECHNOLOGY WITH ENVIRONMENTAL V ARIATION ADAPTABILITY 11479
where p(m/lscript)is the priori probability of the grid m/lscriptof
being occupied by obstacles. However, there is no information
regarding to the occupancy status of grid before mapping,
hence p(m/lscript)will be set to a ﬁxed value of 0.5. In order to
enhance the numerical accuracy and calculation speed in (9),
its logarithm is obtained by :
l(1:k)
/lscript=l(k)
/lscript+l(1:k−1)
/lscript−l/lscript (10)
where lrepresents the logarithm of the odds ratio p/(1−p):
l(1:k)
/lscript≡logp(m/lscript|z(1:k))
1−p(m/lscript|z(1:k)),
l(k)
/lscript≡logp(m/lscript|z(k))
1−p(m/lscript|z(k)),l/lscript≡logp(m/lscript)
1−p(m/lscript)(11)
In (9) and (10), the inverse sensor model p(m/lscript|z(k))is
the key to correlating p(m/lscript|z(1:k−1))with p(m/lscript|z(1:k)),w h i c h
indicates the conﬁdence level of the grid m/lscriptbeing occupied
under the current sensor observation, z(k). The occupancy grid
map can be updated according to (9) with the decision of the
inverse sensor model.
C. Inverse Sensor Model
The inverse sensor model describes the conﬁdence level of
the grid being occupied under the sensor measurement. In thecase of LiDAR, it scans the environment with several laser
beams in the space. The returning point of the laser beam is
where the obstacle is located, and the space penetrated by thelaser beam before returning is the area that is not occupied
by the obstacle. For those grids on the grid map that are
penetrated by the laser beam, the conﬁdence level of the grids
to be occupied by obstacle is a function between “the scan
distance of LiDAR” and “the distance between the grid andthe position of LiDAR”. On the other hand, the grid that is
not penetrated by the laser cannot be determined whether it is
occupied by the obstacle, so its conﬁdence level is 0.5. Thefollowing formula is based on the above discussion :
p(m
/lscript|z(k))
=/braceleftBigg
f(d(k)
/lscript,r(k)
i),ifm/lscriptis on the i-th laser beam.
1/2, otherwise(12)
where p(m/lscript|z(k))represents the conﬁdence level of the grid
m/lscriptbeing occupied under the observation of z(k)at the kth
scan. If m/lscriptis located on the ithlaser beam, then d(k)
/lscriptis the
distance between m/lscriptand the position of LiDAR, and r(k)
iis
the scan distance of the ithlaser beam, as shown in Fig. 8. The
conﬁdence level of the grid m/lscriptin occupancy is the function
between d(k)
/lscriptandr(k)
i, and this function should describe the
conﬁdence level of the grid with the distribution of grids d(k)
/lscript
at the LiDAR scan distance r(k)
i.
In this study, two LiDAR inverse sensor models are
designed, one is the Gaussian distribution model (13), and
Fig. 8. The geometric relationship between the observed grids in the map
and the LiDAR laser beam.
Fig. 9. The Gaussian distribution model with C=1.0 and the distribution
of laser beam on grids with scan distance of 20 mm in static LiDAR.
the other is Quadratic distribution model (14):
pG(d,r)
=⎧
⎪⎨
⎪⎩1+C
2exp/parenleftbigg
−(d−r)2
2σ2/parenrightbigg
,d≤r+σ√
2l n(1+C)
1
2, d>r+σ√
2l n(1+C)
(13)
pQ(d,r)
=⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩1
21
(r−σ)2d2, d∈(0,r−σ]
1
2/bracketleftbigg
1+C/parenleftbigg
1−1
σ2(d−r)2/parenrightbigg/bracketrightbigg
,d∈[r−σ,r+σ]
1
2, d>r+σ
(14)
where σis the standard deviation of Gaussian distribution,
andC∈(0,1)is the scaling factor used for adjusting the
peak value of inverse sensor model, to determine the cognitive
speed of the occupied grid. The design of the Gaussian
distribution model (13) is based on the noise property of
Gaussian distribution in the LiDAR laser ranging, as shown
in Fig. 9. The grids that are located outside of the scandistance rcannot be penetrated by the laser beam and cannot
be determined whether they are occupied or not; hence, their
conﬁdence level is 0.5.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on July 15,2022 at 16:19:25 UTC from IEEE Xplore.  Restrictions apply. 11480 IEEE SENSORS JOURNAL, VOL. 19, NO. 23, DECEMBER 1, 2019
Fig. 10. Comparison of the quadratic distribution model and Gaussian
distribution model.
Fig. 11. Comparison of Gaussian distri bution model and quadratic distribu-
tion model on the grid map.
The Quadratic distribution model (14) refers to the concept
of [2] and (13), being that the main difference from (13)is that the calculation of the Quadratic distribution model is
simpler than the Gaussian distribution model. According to the
characteristics of Quadratic distribution model, its cognition
to the unoccupied grids in the area within the scan distance r
is relatively conservative to the Gaussian distribution model.As shown by the red circle part in Fig. 10, since the prob-
ability of quadratic model (orange line) is higher than the
probability of Gaussian model (blue line), the update rate forthe unoccupied grid by using the Quadratic model is slower
that the one obtained by the Gaussian model. The associated
simulation comparison can be seen in Fig. 11. It shows that
the unoccupied grid will be recognized slower in the Quadratic
model, but its reliability is relatively higher than the Gaussiandistribution model once the grid is conﬁrmed as an unoccupied
space. However, due to its slower update rate, it may lead
to an unstable SLAM when a robot suddenly goes into acorridor. To enhance its adaptability against to the change of
environment, in the next section we are going to present an
environment detection algorithm such that the Cvalue is going
to be updated accordingly.
IV . P
REPROCESSING OF SCAN POINTS
In the algorithm of localization and mapping, the pose of
the sensor is estimated from the matching and alignment in
Fig. 12. Unstable scan points. (the scan area is located on the 3rd ﬂoor of
the Department of Aeronautics and Astronautics, DAA).
Fig. 13. The area where uns table scan points occur.
ICP between the points on the map and the LiDAR scan
points; hence, the scan quality of LiDAR will affect the resultof the points alignment. However, the inﬂuence on the pose
estimation of the sensor will be reduced if these unstable scan
points can be discarded before the ICP process.
A. Unstable Points Removal
The scan quality of LiDAR on the scan points depends on
the scan of the laser beam’s reﬂection. In the radial scan of
laser beam, the scan boundary that is blocked by obstacles
has a gap, and the scan quality of laser beam in this areais less stable: the laser beam may hit the boundary of the
obstacle in front or in contrast, the background in the back.
The received data from the scan distance varies between thetwo scan boundaries. In other words, the scan points in this
region will move along the radial direction, as demonstrated
by the pink oval area in Fig. 12 and Fig. 13.
The connection direction of scan points in the area where
these unstable scan points occur is similar to the emission
direction of the laser. Therefore, the angle between the two
directions needs to be calculated in order to search for the
scan points in this area. On the basis of Fig. 14, there aretwo connection directions for the i
thscan pointBpi:: the ﬁrst
being that it can be connected with the previous scan point
Bpi−1or with the next scan pointBpi+1; and the second one
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on July 15,2022 at 16:19:25 UTC from IEEE Xplore.  Restrictions apply. CHEN AND PENG: ROBUST 2D-SLAM TECHNOLOGY WITH ENVIRONMENTAL V ARIATION ADAPTABILITY 11481
Fig. 14. Geometric diagram of the scan angles.
Fig. 15. Use the mechanism of detecting small scan angle to discard unstable
scan points.
is that the emission direction of laser beam forBpi, is parallel
to its position vector in the sensor coordinate.
On the basis of the scan angles in the geometric diagram
of Fig. 14, the forward and backward scan angles can be
calculated from the coordinates of scan pointsBpi,Bpi+1and
Bpi−1:
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩/Delta1θ
i,fo rward=cos−1/vextendsingle/vextendsingle/vextendsingle/vextendsingleBpT
i
||Bpi||Bpi+1−Bpi
||Bpi+1−Bpi||/vextendsingle/vextendsingle/vextendsingle/vextendsingle
/Delta1θ
i,backward=cos−1/vextendsingle/vextendsingle/vextendsingle/vextendsingleBpT
i
||Bpi||Bpi−Bpi−1
||Bpi−Bpi−1||/vextendsingle/vextendsingle/vextendsingle/vextendsingle(15)
where the absolute value of arcco sine is for obtaining the acute
angle of scan angle. If the forward and backward scan angles
/Delta1θ
i,fo rwardand/Delta1θi,backwardare too small for the ithscan
pointBpi, it will be regarded as an unstable scan point and
will be discarded :
DiscardBpiif
/Delta1θi,fo rward</Delta1 θ Thdor/Delta1θi,backward</Delta1 θ Thd (16)
where /Delta1θThd is the threshold of the scan angle. More scan
points will be discarded as the value of /Delta1θThdincreases. With
the application of (15) and IV-B, these unstable scan points canbe found and discarded. This approach has excellent judging
results especially when in an envi ronment formed of structures
of beams and columns, as shown in Fig. 15.
B. Corridor Detector
As pointed out by [1], environment recognition is also one
of the challenging points for the robust SLAM. The features
Fig. 16. The mechanism of unstable point detection causes excessive removal
of scan points in the distance of corridor.
of the point cloud distribution in a room or in a corridor are
different. Usually, a sophisticated model or a feature identiﬁer
is needed to ﬁgure out if a robot is moving in a corridor
or elsewhere. Moreover, ther e is no strong link between the
parameters adaption and the corridor detection. Therefore,
in the following section, a corridor detector is proposed and thelink for unstable point removal, mapping probability on-line
adjustment as well as loop closured are made.
Based on the aforementioned section, it has been shown
that the mechanism of detecting and discarding scan points
with small scan angle in (15) and IV-B may cause incorrect
outlier ﬁltering results in the corridor environment, as shown
in Fig. 16. The cause is that the geometric characteristics
of far scan points in the corridor are similar to the ones ofunstable scan points: the angle between the wall direction in
a corridor and the emission direction of laser will reduce with
the increasing of the scan distance.
As shown in Fig 17, if LiDAR is located in the middle of a
corridor with a width of 2 d, then the distance between LiDAR
and the wall in corridor will be d. For the scan point that is
in the front of corridor with a distance of d×cot(/Delta1θ
Thd), its
scan angle is equal to /Delta1θThd, as shown in Fig 17(b). For the
scan point that is farther than the distance of d×cot(/Delta1θ Thd),
its scan angle will be less than /Delta1θThd. For example, if LiDAR
is located in the middle of a corridor with a width of 2 meters,the scan point on the wall that is in the front of corridor with
a distance of 20 meters will be discarded under 3 degrees of
/Delta1θ
Thd.I f/Delta1θThd is set to 10 degrees, the scan point on the
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on July 15,2022 at 16:19:25 UTC from IEEE Xplore.  Restrictions apply. 11482 IEEE SENSORS JOURNAL, VOL. 19, NO. 23, DECEMBER 1, 2019
Fig. 17. The geometric relationship between the mechanism of discarded
scan points and corridor.
Fig. 18. A diagram for the corridor detection.
wall only in 6 meters will be reserved, as shown in Fig. 16.
Excessive removal of scan points in the corridor will give rise
to incorrect pose estimation.
In order to avoid the situation occurred in Fig. 16, it is
necessary to determine whether LiDAR is located in the
corridor based on the distribution shape of the current scan
point, and then adjust the threshold /Delta1θThdin IV-B.
If the point distribution is in an elongated shape, the varia-
tion of the maximum of varied direction v∗
1of the points will
be much greater than the varia tion of the minimum varied
direction v∗
2, as shown in Fig. 18. The projection amount on
the axis that go through the center of the points pcalong the
direction v∗
1, is much more than the projection amount on the
v∗
2axis. The ratio between the two indicates the elongated
degree of the distribution shape of points. On the basis ofabove discussion, we can deﬁne the indicator parameter of
corridor η
Corridor in (17):
ηCorridor ≡max
/bardblv/bardbl=1/summationtextN
i/bracketleftbig
vT(Bpi−pc)/bracketrightbig2
min
/bardblv/bardbl=1/summationtextN
i/bracketleftbigvT(Bpi−pc)/bracketrightbig2
=/summationtextN
i/bracketleftbigv∗
1T(Bpi−pc)/bracketrightbig2
/summationtextN
i/bracketleftbig
v∗
2T(Bpi−pc)/bracketrightbig2(17)where pcis the center of points :
pc=¯p≡1
N/summationdisplay N
iBpi (18)
The solution of (17) is computed from the PCA. The
directions v1and v2of the principal component can be
obtained by calculating the covariance matrix /Sigma1with diagonal
decomposition :
/Sigma1≡/summationdisplay N
i(Bpi−¯p)(Bpi−¯p)T>0(/Sigma1=/Sigma1T)(19)
which can be re-represented by
/Sigma1=Q/Lambda1Q−1=Q/Lambda1QT,(Q−1=QTfor/Sigma1=/Sigma1T)(20)
where/braceleftBigg
/Lambda1=diag(λ1,λ2), λ 1>λ 1>0
Q=/bracketleftBig
v1v2/bracketrightBig
,||v1|| = || v2|| = 1,vT
1v2=0(21)
The principal component direction of the points v1andv2are
the solution for v∗
1andv∗
2:
v∗
1=arg max
||v||=1/summationdisplay N
i/bracketleftBig
vT(Bpi−¯p)/bracketrightBig2
=v1 (22)
v∗
2=arg min
||v||=1/summationdisplay N
i/bracketleftBig
vT(Bpi−¯p)/bracketrightBig2
=v2 (23)
The indicator parameter of corridor ηCorridor can be obtained
by substituting (22) and (23) into (17):
ηCorridor ≡max
||v||=1/summationtextN
i/bracketleftbigvT(Bpi−pc)/bracketrightbig2
min
||v||=1/summationtextN
i/bracketleftbig
vT(Bpi−pc)/bracketrightbig2=λ1
λ2(24)
Eq. IV-C shows that the value can be used to evaluate if
a robot is going into a corridor like environment and SLAM
parameters can then be modiﬁed accordingly.
C. Adaptive Threshold Design
Threshold of scan angle /Delta1θThd should be automatically
tuned according to the indicator ηCorridor of distribution shape
of scan points. When LiDAR is located in corridor, /Delta1θThd
should be adjusted lower to avoid excessive removal of scan
points in the corridor. As for the programming of ηCorridor
and/Delta1θThd,t h eηCorridor is mapped to the angular indicator
θCorridor so that its unit is consistent with /Delta1θThd:
θCorridor ≡tan−1 1
ηCorridor(25)
Therefore, the range [1,∞)of the corridor indicator
ηCorridor is mapped to 45 ∼0d e g r e eo f θCorridor . On the basis
of attempted experience, the mapping relationship between
θCorridor and/Delta1θThdis designed to be as follows :
/Delta1θThd≡⎧
⎪⎪⎨
⎪⎪⎩3◦ifθCorrdor ≤25◦
7
15θCorrdor −26
3◦
,ifθCorrdor ∈(25◦,40◦]
10◦, ifθCorrdor >40◦
(26)
From (26), we can ﬁnd that θCorridor is close to 0 degrees
when the scan points are distributed in an elongated shape
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on July 15,2022 at 16:19:25 UTC from IEEE Xplore.  Restrictions apply. CHEN AND PENG: ROBUST 2D-SLAM TECHNOLOGY WITH ENVIRONMENTAL V ARIATION ADAPTABILITY 11483
Fig. 19. The adaptive threshold of scan angle is designed on the basis of
the anglular indicator of corridor.
Fig. 20. Corridor detection on the scan point distribution using principal
component analysis.
and the setting of /Delta1θThd is reduced to 3 degrees; θCorridor
will go up to 45 degrees when the scan points are distributed
in a square shape, and the setting of /Delta1θThd is increased
to 10 degrees until it reaches saturation point. The mappingrelationship is as shown in Fig. 19.
With (25), the distribution shape of the scan points can
determine whether they are distributed in an elongated shape.Based on this result, threshold of scan angle /Delta1θ
Thd can
be adjusted to proper value. For example, θCorridor is
2.26 degrees in corridor, as shown in Fig. 20, and 3 degrees
of/Delta1θThdcan be obtained from (26). Thus, more scan points
in corridor can be kept, as shown in Fig. 21.
In addition, further on we demonstrate how the corridor
detector can be used to enhance the environment adaptability
during the SLAM. Since there are only few features in thecorridor environments, as illustrated in Fig. 16, it is easy to
cause SLAM failures.
Consider the case in which the robot is moving from a
room and then steps into a corridor. The dimensions of the
corridor are 1.8 meters in width and 35 meters in depth.
The features are monotonous and there aren’t any beams
or columns in such environment. Therefore, the corridor
detector is applied to evaluate whether the robot is locatedin a corridor or not. Once the robot recognizes its location
is indeed a corridor and the environment has not yet been
explored, the ICP correspondence matching threshold d
THD
Fig. 21. Unstable point removal based on the adaptive threshold of scan
angle with corridor detection.
Fig. 22. SLAM result without the use of corridor detector based sensor
model adaption law.
Fig. 23. SLAM result with the aid of co rridor detector based sensor model
adaption law.
and the mapping factor Cused in the sensor model (14) are
going to be set by 150 mm and 0.6, respectively. Fig. 22 isthe experimental result by using ﬁxed SLAM parameters.
Since the corridor cannot be constructed in a short period of
time, it gives rise to mapping divergence. On the contrary,as demonstrated in Fig. 23, applying the corridor detector
makes it able to adapt to the variation of the environment
and to accelerate the recognition speed of the two sides ofthe corridor as well as the bottom wall, then a successful
SLAM can be achieved. Therefore, experiments show that the
corridor detector provides readily usable information about
the type of environment (i.e., room versus corridor) and
then changes the mapping probability accordingly. Fig. 23(b)shows that the result of the map construction is not com-
pletely closed, which can be further improved by using loop
closure.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on July 15,2022 at 16:19:25 UTC from IEEE Xplore.  Restrictions apply. 11484 IEEE SENSORS JOURNAL, VOL. 19, NO. 23, DECEMBER 1, 2019
Fig. 24. The cumulative errors in the process of localization and mapping
will results in overlay error. (the scan area is located on the 3rd ﬂoor of DAA).
V. L OOP CLOSURE
Both pose estimation error and mapping error occurred
in the process of localization and mapping algorithm. These
errors will affect the subsequent localization and mapping,
and the cumulative errors will result in overlay error, which
makes the same obstacle inconsistently mapped on the map.
For example, the scan on the 3rd ﬂoor of DAA as shownin Fig. 24 (a): we started from the laboratory and went
around the 3rd ﬂoor in a clockwise direction, and then back
to the laboratory. In the process of localization and mappingalgorithm, the mapped wall when leaving the laboratory and
returning to the laboratory is inconsistently shown on the map,
a ss h o w ni nF i g .2 4( b ) .
This kind of deformed and non-closed map can be divided
into several submaps for shifting and adjustment, so thatthe map can be more freely to correct the non-closed area
in the map and estimated path of sensor in each submap.
The correction mechanism for this is called “loop closure”,by which the cumulative errors in localization process can be
eliminated.
A. Map Segmentation
In the process of loop closure, it is necessary to ensure
the correctness of shapes of each submap and the connection
between the maps. First, the estimated scan path obtained
from SLAM algorithm is cut into several pieces with ﬁxedlength, and several grid maps can be reconstructed along those
paths, as shown in Fig. 25 and Fig. 26. Each submap is
generated adaptively according to the traveling distance as well
asη
Corridor . To put it clearly, when the traveling distance is
greater than 7 meters and ηCorridor <10, one submap is going
to be issued. Note that a large value of ηCorridor indicates
that the system is now in a corridor like environment. Such
environment usually lacks of solid feature points and is notadequate for loop closure. Hence, the corridor detector is able
to evaluate the adequacy of the submap generation. Finally,
the occupied grids of each grid map are chosen as the feature
points of submap for the loop closure.
B. Pose Correction of Submaps
In the loop closure, all the submaps must be rotated and
shifted except the ﬁrst submap. That means the pose of all
Fig. 25. Map segmentation for submap s. (localization and mapping result
from the 3rd ﬂoor of DAA).
Fig. 26. Submaps without loop closure. (localization and mapping result
from the scan data on the 3rd ﬂoor of DAA).
submaps will be readjusted. In plane motion, the rotation has
one degree of freedom (yaw angle ψ)and the translation
has two degrees of freedom (translation vector t); hence,
the solution for the pose of all submaps (with a total of n
submaps) is as follows :
x=[ψ(2)t(2)Tψ(3)t(3)T... ψ(n)t(n)T]T
where
t(i)=[t(i)
xt(i)
y]T(27)
where the superscript represents the index of submap. The
goal of loop closure is to adjust all the submaps so that
the corresponding feature points between maps can coincide.
Therefore, the optimization issue is as follows :
x∗=arg minx/summationdisplayn
i=2/summationdisplay N(i)
j=1/vextenddouble/vextenddouble/vextenddoubleT(p(i)
j)−q(i)
j/vextenddouble/vextenddouble/vextenddouble2
(28)
T(p(i)
j)=R(i)p(i)
j+t(i),
R(i)=/bracketleftbiggcosψ(i)−sinψ(i)
sinψ(i)cosψ(i)/bracketrightbigg
,p(i)
j=/bracketleftBigg
p(i)
x,j
p(i)
y,j/bracketrightBigg
(29)
where pj(i)represents the jthofN(i)feature points in the ith
submap and Trepresents the transformation of translation and
rotation. qj(i)represents the matchin g feature points in other
submaps for pj(i). The approach is to determine the closest
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on July 15,2022 at 16:19:25 UTC from IEEE Xplore.  Restrictions apply. CHEN AND PENG: ROBUST 2D-SLAM TECHNOLOGY WITH ENVIRONMENTAL V ARIATION ADAPTABILITY 11485
Fig. 27. Loop closure effect. (localization and mapping result from the scan
data on the 3rd ﬂoor of DAA).
Fig. 28. Map reconstructure with th e aid of corridor detector based loop
closure.
point to pj(i)in other submaps after translation and rotation
(the point pairs that are too far apart will not be included inthe calculation), which is :
q(i)
j=/braceleftBigg
T(p(k)
/lscript),k/negationslash=1
p(1)
/lscript, else,which is the closest (30)
point to T(p(i)
j)in other submaps ((30)
As (28) is a nonlinear optimization issue which can be
resolved by Levenberg-Marquardt (LM) algorithm [40], aniterative method, and the closest point in (30) should be
updated in each process of iteration.
LM algorithm can be used to solve x
∗, the best pose of
submaps, and carries out the loop closure, as shown in Fig. 27.
After the process of loop closure, the dislocated wall will becoincident on the map and the LiDAR scan locus will also be
corrected (green locus in the diagram).
The loop closure algorithm is also applied to Fig. 23(b),
where the detail mismatched map stitching result is shown
in Fig. 28. Similarly, applying the proposed corridor detector
based loop closure can achieve good mapping recovery as
illustrated in Fig. 29
VI. E
XPERIMENT VALIDATION AND COMPARISON
In order to verify the performance of the proposed algo-
rithm, a couple of experiments were considered. Note that
no odometry information was used for the following SLAM
experiment.
First, an experiment was conducted around the DAA-3F,
where the developed SLAM module was equipped on a RC
car as shown in Fig. 30 and three rounds LiDAR scans wereFig. 29. Corridor detector based subm ap generation for DAA-2F and its
loop closure result.
Fig. 30. DAA-3F experimental scene.
Fig. 31. SLAM performance comparison (DAA-3F).
collected. The collected dataset was also fed into the well
known Hector SLAM [24] provided by the robot operation
system (ROS). Compared with the Hector SLAM, Fig. 31 evi-
dently shows that the proposed algorithm can provide morerobust performance in localization and mapping.
As for the localization result of SLAM, the precision of
the proposed SLAM needs to be further veriﬁed. Therefore,
the localization results must be compared with the ground
truth. This study also utilizes the scan data and groundtruth [41], [42] provided by the Computer Science and Arti-
ﬁcial Intelligence Laboratory (CSAIL) at the Massachusetts
Institute of Technology (MIT), to verify the SLAM algorithm.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on July 15,2022 at 16:19:25 UTC from IEEE Xplore.  Restrictions apply. 11486 IEEE SENSORS JOURNAL, VOL. 19, NO. 23, DECEMBER 1, 2019
Fig. 32. Floor plan and the ground truth for scan data 2012_01_25_12_14_25.
TABLE I
SCAN DATA INFORMATION AT MIT S TATA CENTER
The selected scan data “ 2012_01_25_12_14_25”, as shown
in Table 1. The scan process was carried out for about
20 minutes on the second ﬂoor of the Stata Center on
January 25, 2012. The scan path can be referred to Fig. 32,and the entire scan path is about 350 m. The LiDAR used
for this scan is Hokuyo UTM-30LX with a scan frequency
of 20 Hz.
The following experiment uses the scan data from CSAIL
to test SLAM algorithm with various parameter settings. The
respective results and the ground truth provided by CSAIL
will be used to calculate tracking errors (distance between
the position of ground truth and estimated position) and yawangle errors, and the advantages and disadvantages of various
parameter selection will also be compared.
A. Distance Threshold of Matching Points in ICP
A localization precision comparison for the ICP distance
threshold of 100, 150, 200 and 300 mm is carried out on the
basis of the parameter settings in Table 2. From Fig. 33 andTable 3, it shows that the strict distance threshold of the
matching points is beneﬁcial for the removal of matched
outlier which will reduce the localization error.
B. Weight of Rigid Alignment in ICP
A localization precision comparison for the weight of rigid
alignment in ICP such as uniform weight ( w
i=1,∀i),
distance weightRwiin (7), and inverse weightIwiin III is
carried out on the basis of the parameter settings in Table 2.The result in Fig. 34 indicates th at the localization error of
uniform weight starts to diverge at about 640 seconds of scan
time. On the basis of Table 4, the tracking error of distance
Fig. 33. Localization error with differen t settings of distance threshold of
ICP matching point.
TABLE II
PARAMETER SETTING FOR THE BEST EXPERIMENTS
TABLE III
LOCALIZATION ERROR WITHDIFFERENT SETTINGS OF
DISTANCE THRESHOLD OF MATCHI NG POINTS
weightRwiand inverse weightIwiare below 30 cm. As for
the distance weight at 780 seconds with 40 degree/sec ofLiDAR rotation, there are four scans that are not aligned
with the map and results in 6 degree of yaw angle error.
In contrast, the inverse weight
Iwiis more stable on the
estimation of the yaw angle. It is discovered that the weighting
design in III focuses more on the scan points far from LiDAR.
Thus, the inﬂuence of near scan points on pose estimation is
reduced. Therefore, the scan points far from LiDAR are still
able to align with the matching points in the map with thefaster movement of LiDAR, and increase the bandwidth of
pose estimation of the sensor.
C. Inverse Sensor Model and Scale Factor
The localization precision of different inverse sensor model
with the scaling factor Cof inverse sensor model is tested
on the basis of the parameter setting in Table 2. If Gaussian
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on July 15,2022 at 16:19:25 UTC from IEEE Xplore.  Restrictions apply. CHEN AND PENG: ROBUST 2D-SLAM TECHNOLOGY WITH ENVIRONMENTAL V ARIATION ADAPTABILITY 11487
Fig. 34. Localization error with differ ent settings of rigid alignment weight
during the scanning process.
TABLE IV
LOCALIZATION ERROR WITHDIFFERENT SETTINGS
OFRIGID ALIGNMENT WEIGHT
Fig. 35. Localization error with different scaling factor of Gaussian
distribution model during the scanning process.
distribution model is used as the inverse sensor model for
map construction with the setting Cof 0.3, 0.5, and 1.0,
the localization error will occur as shown in Fig. 35 and
Table 5. The localization results indicate that localization
divergence occurred at 640 seconds and 860 seconds, withdifferent setting of Cfor Gaussian distribution model.
If the quadratic distribution model is used with the scaling
factor settings of 0.3, 0.5, and 1.0, the localization error willTABLE V
LOCALIZATION ERROR WITHDIFFERENT SCALING FACTOR OF
GAUSSIAN DISTRIBUTION MODEL
Fig. 36. Localization error with different scaling factor of quadratic
distribution model.
TABLE VI
LOCALIZATION ERROR WITHDIFFERENT SCALING
FACTOR OF QUADRATIC DISTRIBUTION MODEL
occur as shown in Fig. 36. With the setting of C=1.0i n
the quadratic distribution model, the overly fast environment
recognition will reduce the robustness of the map, which
will affect the localization pr ecision. The localization error
is signiﬁcantly accumulated in t he process, which will result
in localization failure. According to Fig. 36 and Table 6 ,
the better setting for quadratic distribution model is C=0.3
with the tracking error less than 30 cm.
On the basis of above experiment, the scaling factor Cof the
Gaussian distribution model in (13) and quadratic distribution
model in (14) can be respectively set to 0.5 and 0.3 under the
condition of 20 Hz scan updating rate, 20 mm grid resolutionand width of distribution model σ=20 mm.
As for determining whether the grids are occupied in the
map, the recognition speed in quadratic distribution model is
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on July 15,2022 at 16:19:25 UTC from IEEE Xplore.  Restrictions apply. 11488 IEEE SENSORS JOURNAL, VOL. 19, NO. 23, DECEMBER 1, 2019
Fig. 37. Localization error with differ ent settings for the threshold of scan
angle during the scanning process.
TABLE VII
LOCALIZATION ERROR WITHDIFFERENT SETTINGS
FOR THE THRESHOLD OF SMALL SCAN ANGLE
slower than in the Gaussian distribution model; in contrast,
once the grids are veriﬁed to be occupied, the former’s reli-
ability and robustness are higher than the latter’s. Therefore,
the quadratic distribution model is chosen.
D. Threshold of Unstable Scan Angle
Based on the parameter setting in Table 2, a localization
precision test with the scan angle threshold settings /Delta1θThd
of 0, 3, 5 degrees and the adaptive threshold in (26) is carried
out. From the localization error comparison for /Delta1θThd in
Fig. 37 and Table 7 , it can be seen that localization with
the threshold of 5 degrees starts to fail after 640 seconds.
As LiDAR is going through the corridor with dimensionof 13.2 m long and 1.8 m wide at the time, the removal
mechanism of unstable scan points discards the scan point that
is 10.3 m away from the corridor, which causes the failure of
translation estimation. On the basis of all localization results,
it can be seen that the self-adjusting threshold (26) effectivelydiscards unstable scan points and reduces the pose estimation
error, as well as avoids excessively discard of scan points in
the corridor.
E. Loop Closure
The localization and mapping algorithm has a maximal
tracking error of about 30 cm, w ith the parameter setting
in Table 2. Therefore, the deviation exists in between the
Fig. 38. A overview of SLAM result compared with ground truth.
Fig. 39. Detail of SLAM result compared with ground truth.
TABLE VIII
LOCALIZATION ERROR BEFORE AND AFTER LOOP CLOSURE
estimated scan path and the ground truth path, as well as in
between the constructed map and the ﬂoor plan, as shown
in Fig. 38, Fig. 39. In order to correct this error, submaps
are created in the interval of every 5 m along the estimated
scan path based on the localization and mapping result inFig. 40 (a), as shown in Fig. 40 (b). The loop closure is also
carried out to allow the map to adjust its geometric shape and
correct the cumulative errors in localization and mapping.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on July 15,2022 at 16:19:25 UTC from IEEE Xplore.  Restrictions apply. CHEN AND PENG: ROBUST 2D-SLAM TECHNOLOGY WITH ENVIRONMENTAL V ARIATION ADAPTABILITY 11489
Fig. 40. (a) SLAM result and (b) segmentation of submap.
Fig. 41. Detailed localizatio n result after loop closure.
Fig. 42. Localization error before and af ter loop closure during the scanning
process.
After loop closure, the localization error and mapping error
are corrected to some extent, as shown in Fig. 41. From the
localization error comparison shown in Fig. 42 and Table 8,it can be seen that the maximum of tracking error is reduced
from about 30 cm to 23 cm after the loop closure.
VII. C ONCLUSION AND FUTURE WORK
This paper presents a robust 2D SLAM technology. The
proposed method is capable of adapting to uncertain envi-
ronment variations and adjus ting SLAM parameters auto-
matically. To enhance the real-time computation efﬁciency,an analytic solution of ICP is also carried out. With the aid of
the corridor detector, grid map occupancy pr obability update,
unstable points removal as well as the submap generation,a robust 2D SLAM can be achieved. In addition to our own
experimental tests, we also considered the data set from MIT
Stata Center dataset to test the localization precision of algo-
rithm. As for the precision test for localization and mapping,
the test results of the proposed SLAM are summarized asfollows: the maximum tracking error is about 30 cm, from
the scan data in the route up to about 350 m; and after
the loop closure, the maximum error can be attenuated to20 cm. Finally, since the proposed SLAM algorithm considers
a single LiDAR only, it reduces the development cost and
system integration effort signiﬁcantly. The relative application
is not only limited to wheel robotics but for portable mapping
devices as well. Experiments are taken into consideration inorder to verify the feasib ility of the pr oposed method.
A
PPENDIX A
ANALYTIC SOLUTION OF RIGID BODY ALIGNMENT
Assume that there are two sets of two-dimensional points
Pand Q, which are composed by pi=[px,ip y ,i]T
and qi=[qx,iq y,i]Trespectively ( i=1∼N).A f t e r
the rotating and shifting points set P,piand qishould
coincide. The relationship between the two sets of points can
be expressed as follows :
qi=Rpi+t (31)
where Ris a rotation matrix and is also a 2-by-2 orthogonal
matrix; tis a 2-by-1 translation vector. We want to solve R
andton the basis of the given matching point piandqifrom
the points set PandQ. The optimization problem is described
as follows :
min
R,tf(R,t)subject to RTR=I,
f(R,t)=/summationdisplay N
iwi/vextenddouble/vextenddouble(Rpi+t)−qi/vextenddouble/vextenddouble2(32)
where f(R,t)represents the cost function of and wiis the
weight coefﬁcient of the matching point piandqi. Eq. (32)
describes that the best solution for Rand tresults in the
smallest sum of squares of the distances between the matching
points after rigid motion. In order to solve this problem,
we ﬁrst expand the cost function f:
f(R,t)=/summationdisplay N
iwi/vextenddouble/vextenddouble(Rpi+t)−qi/vextenddouble/vextenddouble2
=/parenleftbigg/summationdisplay N
iwi/parenrightbigg
tTt+2/bracketleftbigg/summationdisplay N
iwi(pT
iRT−qT
i)/bracketrightbigg
t
+/summationdisplay N
iwi(pT
iRTRpi+qT
iqi) (33)
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on July 15,2022 at 16:19:25 UTC from IEEE Xplore.  Restrictions apply. 11490 IEEE SENSORS JOURNAL, VOL. 19, NO. 23, DECEMBER 1, 2019
It can be seen from (33) that the cost function fis a positive
deﬁnite quadratic function of t, therefore the extremum of f
fortmust be the global minimum.
Differentiating fwith respect to tgives:
∂f
∂t/vextendsingle/vextendsingle/vextendsingle/vextendsingle
t=t∗=2/parenleftbigg/summationdisplay N
iwi/parenrightbigg
t∗+2/summationdisplay N
iwi(Rpi−qi)=0
(34)
Rearranging (34), optimal translation vector t∗can be
obtained as follows :
t∗=¯q−R¯p (35)
where ¯pand¯qare the centroids (or center of mass) of the
points sets PandQrespectively :
¯q≡/summationtextN
iwiqi
/summationtextN
iwi,¯p≡/summationtextN
iwipi
/summationtextN
iwi(36)
Therefore, as long as the obtained t∗has a relation with R
as seen in (35), the cost function fwill reach its minimum.
Substitute the relation formula (35) into the cost function (33),replace the variable t, and get the following :
f(R,t
∗)=f(R,¯q−R¯p)=/summationdisplay N
iwi/bardblRxi−yi/bardbl2(37)
where xiandyiare deﬁned as the coordinates for the corre-
sponding centroid ¯pand¯q:
/braceleftBigg
xi≡pi−¯p
yi≡qi−¯q(38)
Try to expand (37) again to get :
f(R,t∗)=/summationdisplay N
iwi/bardblRxi−yi/bardbl2
=/summationdisplay N
iwixT
ixi−2/summationdisplay N
iwiyT
iRxi+/summationdisplay N
iwiyT
iyi
(39)
In (39), only/summationtextN
iwiyT
iRxivaries with the variable R,s ot h e
original optimization problem in (32) can be transformed into :
max
RTR=I/summationdisplay N
iwiyT
iRxi,/braceleftBigg
xi≡pi−¯p
yi≡qi−¯q(40)
The rotation matrix Rcan be solved according to (40). After
the optimal rotation matrix R∗is solved, then the optimal t∗
can be obtained based on (35).
In order to solve (40), it is necessary to expand R,xiand
yiin the formula. As for the planar rigid motion, only the yaw
angle ψhas freedom of rotation, which is :
R=/bracketleftbiggcosψ−sinψ
sinψ cosψ/bracketrightbigg
(41)
In addition, the 2-by-1 vector of points coordinate xiand
yiare:
xi=/bracketleftbiggxx,i
xy,i/bracketrightbigg
,yi=/bracketleftbiggyx,i
yy,i/bracketrightbigg
(42)
After substituting (41) and (42) into (40), we can get :
/summationdisplay N
iwiyT
iRxi=cosψ(s11+s22)+sinψ(s12−s21)(43)where s11,s12,s21ands22are from the cross-covariance matrix
ofpiandqiwith the weight wi:
S=/bracketleftbiggs11 s12
s21 s22/bracketrightbigg
≡/summationdisplay N
iwi(qi−¯q)(pi−¯p)T
=/bracketleftbigg/summationtextN
iwixx,iyx,i/summationtextN
iwixx,iyy,i/summationtextN
iwixy,iyx,i/summationtextN
iwixy,iyy,i/bracketrightbigg
(44)
The following algebra substitution is utilized to handle the
trigonometric function in (43):
/braceleftBigg
sinγ=(s11+s22)//Gamma1
cosγ=(s12−s21)//Gamma1(45)
where /Gamma1is the normalization factor :
/Gamma1=/radicalbig
(s11+s22)2+(s12−s21)2 (46)
Substitute (45) into (43), simplify with the angle-sum iden-
tities in sine function, and get the following :
N/summationdisplay
iwiyT
iRxi=/Gamma1sin(ψ+γ) (47)
According to (47),/summationtextN
iwiyT
iRxiwill reach its maximum
when sin (ψ∗+γ)reaches its maximum value of 1. That
is, the optimal yaw angle ψ∗must be complementary to γ,
which is :
sin(ψ∗+γ)=1,asψ∗=/parenleftbigg1
2π−γ/parenrightbigg
+2nπ (48)
where ∀n∈Z. According to (45) and (48), we can calcu-
late the value of sine and cosine function for the best yawangle ψ
∗:
/braceleftBigg
cosψ∗=sinγ=(s11+s22)//Gamma1
sinψ∗=cosγ=(s12−s21)//Gamma1(49)
Substitute (49) into (41) yields the best rotation matrix
estimate R∗:
R∗=1
/Gamma1/bracketleftbiggs11+s22 s21−s12
s12−s21 s11+s22/bracketrightbigg
(50)
Therefore, the calculation process for the entire rigid align-
ment is: calculate the cross-covariance matrix Sin (44) from
the given matching point piand qi, substitute it into (50)
to obtain the best rotation matrix R∗, and then substitute it
into (35) to obtain the translation vector t∗. In this way, the cost
function in (32) reaches its mini mum, and the rigid alignment
between the two points sets PandQis completed.
REFERENCES
[1] C. Cadena et al. , “Past, present, and future of simultaneous localization
and mapping: Toward the robust-perception age,” IEEE Trans. Robot. ,
vol. 32, no. 6, pp. 1309–1332, Dec. 2016.
[2] A. Elfes, “Sonar-based real-world mapping and navigation,” IEEE
J. Robot. Autom. , vol. 3, no. 3, pp. 249–265, Jun. 1987.
[3] A. Elfes, “Using occupancy grids for mobile robot perception and
navigation,” Computer , vol. 22, no. 6, pp. 46–57, Jun. 1989.
[4] J. J. Leonard and H. F. Durrant-Whyte, “Mobile robot localization by
tracking geometric beacons,” IEEE Trans. Robot. Automat. , vol. 7, no. 3,
pp. 376–382, Jun. 1991.
[ 5 ] J .A .C a s t e l l a n o sa n dJ .D .T a r d o s , Mobile Robot Localization and Map
Building: A Multisensor Fusion Approach . Boston, MA, USA: Kluwer,
2000.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on July 15,2022 at 16:19:25 UTC from IEEE Xplore.  Restrictions apply. CHEN AND PENG: ROBUST 2D-SLAM TECHNOLOGY WITH ENVIRONMENTAL V ARIATION ADAPTABILITY 11491
[6] M. Csorba, Simultaneous Localisation and Map Building . Oxford, U.K.:
Univ. Oxford, 1998.
[ 7 ] M .W .M .G .D i s s a n a y a k e ,P .N e w man, S. Clark, H. F. Durrant-Whyte,
and M. Csorba, “A solution to the simultaneous localization and map
building (SLAM) problem,” IEEE Trans. Robot. Autom. , vol. 17, no. 3,
pp. 229–241, Jun. 2001.
[8] J. E. Guivant and E. M. Nebot, “Optimization of the simultaneous
localization and map-building algorithm for real-time implementation,”IEEE Trans. Robot. Autom. , vol. 17, no. 3, pp. 242–257, Jun. 2001.
[9] J. Leonard, H. Durrant-Whyte, and I. J. Cox, “Dynamic map building for
autonomous mobile robot,” in Proc. IEEE Int. Workshop Intell. Robots
Syst., Towards New Frontier Appl. , Aug. 1990, vol. 1. pp. 89–96.
[10] P. Newman, “On the structure and solution of the simultaneous local-
isation and map building problem,” Ph.D. dissertation, Dept. Mech.Mechtron. Eng., Austral. Centre Field Robot., Univ. Sydney, Sydney,
NSW, Australia, Mar. 1999, vol. 41.
[11] S. Williams, G. Dissanayake, and H. Durrant-Whyte, “Towards terrain-
aided navigation for underwater robotics,” Adv. Robot. , vol. 15, no. 5,
pp. 533–549, Jan. 2001.
[12] H. P. Moravec, “Sensor fusion in certainty grids for mobile robots,” AI
Mag. , vol. 9, no. 2, p. 61, 1988.
[13] S. Thrun, “Learning occupancy grids with forward models,” in Proc.
IEEE/RSJ Int. C onf. Intell. Robots Syst. Expanding Societal Role Robot.
Next Millennium , Oct./Nov. 2001, vol. 3, pp. 1676–1681.
[14] J. Borenstein and Y . Koren, “The vector ﬁeld histogram-fast obstacle
avoidance for mobile robots,” IEEE Trans. Robot. Autom. , vol. 7, no. 3,
pp. 278–288, Jun. 1991.
[15] J. Buhmann et al. , “The mobile robot Rhino,” Ai Mag. , vol. 16, no. 2,
p. 31, Jun. 1995.
[16] W. Burgard et al. , “Experiences with an interactive museum tour-guide
robot,” Artif. Intell. , vol. 114, no. 1, pp. 3–55, Oct. 1999.
[17] D. Guzzoni, A. Cheyer, L. Julia, and K. Konolige, “Many robots make
short work: Report of the SRI International mobile robot team,” AI Mag. ,
vol. 18, no. 1, p. 55, May 1997.
[18] S. Thrun et al. , “Probabilistic algorithms and the interactive museum
tour-guide robot minerva,” Int. J. Robot. Res. , vol. 19, no. 11,
pp. 972–999, Nov. 2000.
[19] B. Yamauchi and P. Langley, “Place recognition in dynamic environ-
ments,” J. Robotic Syst. , vol. 14, no. 2, pp. 107–120, Feb. 1997.
[20] B. Yamauchi, P. Langley, A. C. Sc hultz, J. Grefenstette, and W. Adams,
Magellan: An Integrated Adaptive Architecture for Mobile Robotics ,
Institute for the Study of Learning and Expertise, Palo Alto, CA, USA,
1998.
[21] C.-C. Wang, C. Thorpe, S. Thr un, M. Hebert, and H. Durrant-Whyte,
“Simultaneous localization, mappi ng and moving object tracking,” Int.
J. Robot. Res. , vol. 26, no. 9, pp. 889–916, Sep. 2007.
[22] M. Bosse and R. Zlot, “Map matching and data association for large-
scale two-dimensional laser scan-based SLAM,” Int. J. Robot. Res. ,
vol. 27, no. 6, pp. 667–691, Jun. 2008.
[23] G. Grisetti, C. Stachniss, and W. Burgard, “Improved techniques for grid
mapping with rao-blackwellized particle ﬁlters,” IEEE Trans. Robot. ,
vol. 23, no. 1, pp. 34–46, Feb. 2007.
[24] S. Kohlbrecher, O. V . Stryk, J. Meyer, and U. Klingauf, “A ﬂexible and
scalable SLAM system with full 3D motion estimation,” in Proc. IEEE
Int. Symp. Saf., Secur., Rescue Robot. , Nov. 2011, pp. 155–160.
[25] W. Hess, D. Kohler, H. Rapp, and D. Andor, “Real-time loop closure
in 2D LIDAR SLAM,” in Proc. IEEE Int. Conf. Robot. Autom. (ICRA) ,
May 2016, pp. 1271–1278.
[26] K. Konolige, G. Grisetti, R. Kümmerle, W. Burgard, B. Limketkai, and
R. Vincent, “Efﬁcient sparse pose adjustment for 2D mapping,” in Proc.
IEEE/RSJ Int. C onf. Intell. Robots Syst. , Oct. 2010, pp. 22–29.
[27] L. Li and L. Schulze, “Comparison and evaluation of SLAM algorithms
for AGV navigation,” in Proc. 8th Int. Conf. Eng. Manage. , 2018,
pp. 213–221.
[28] R. Yagfarov, M. Ivanou, and I. Afanasyev, “Map comparison of lidar-
based 2D SLAM algorithms using precise ground truth,” in Proc.
15th Int. Conf. Control, Autom., Robot. Vis. (ICARCV) , Nov. 2018,
pp. 1979–1983.
[29] V . Kirnos, V . Antipov, A. Priorov, and V . Kokovkina, “The LIDAR
Odometry in the SLAM,” in Proc. 23rd Conf. Open Innov. Assoc.
(FRUCT) , Nov. 2018, pp. 180–185.
[30] G. Jiang, L. Yin, G. Liu, W. Xi, and Y . Ou, “FFT-based scan-matching
for SLAM applications with low-cost laser range ﬁnders,” Appl. Sci. ,
vol. 9, no. 1, p. 41, Dec. 2018.[31] G. Xu et al. , “Precise point set registration using point-to-plane distance
and correntropy for LiDAR based localization,” in Proc. IEEE Intell.
Vehicles Symp. (IV) , Jun. 2018, pp. 734–739.
[32] S. Z. Mehdi and J.-H. Ryu, “2D pose nodes sampling heuristic for fast
loop closing,” in Proc. Annu. Spring Conf. Korean Soc. Automot. Eng.
(KSAE) , May 2017, pp. 111–115.
[33] R. Opromolla, G. Fasano, M. Grassi, A. Savvaris, and A. Moccia, “PCA-
based line detection from range data fo r mapping and localization-aiding
of UA Vs,” Int. J. Aerosp. Eng. , vol. 2017, Apr. 2017, Art. no. 4241651.
[34] D. Wu, Y . Meng, K. Zhan, and F. Ma, “A LIDAR SLAM based on point-
line features for underground mining vehicle,” in Proc. Chin. Autom.
Congr. (CAC) , Nov./Dec. 2018, pp. 2879–2883.
[35] M. Peter, S. R. U. N. Jafri, and G. V osselman, “Line segmentation of 2D
laser scanner point clouds for indoor sl am based on a range of residuals,”
Remote Sens. Spatial Inf. Sci. , vol. 4, no. 2, pp. 363–369, Sep. 2017.
[36] S. Agarwal, K. S. Parunandi, and S. Chakravorty, “Robust pose-graph
SLAM using absolute orientation sensing,” IEEE Robot. Autom. Lett. ,
vol. 4, no. 2, pp. 981–988, Apr. 2019.
[37] Y .-T. Wang, C.-C. Peng, A. A. Ra vankar, and A. Ravankar, “A single
LiDAR-based feature fusion indoor localization algorithm,” Sensors ,
vol. 18, no. 4, p. 1294, Apr. 2018.
[38] P. J. Besl and N. D. McKay, “Method for registration of 3-D shapes,”
Proc. SPIE , vol. 1611, p. 91, Apr. 1992.
[39] S. Thrun, W. Burgard, and D. Fox, Probabilistic Robotics . Cambridge,
MA, USA: MIT Press, 2005.
[40] D. W. Marquardt, “An algorithm for least-squares estimation of nonlinear
parameters,” J. Soc. Ind. Appl. Math. , vol. 11, no. 2, pp. 431–441, 1963.
[41] MIT Computer Science Artiﬁcial Intelligence Laboratory. MIT Stata
Center Data Set . Accessed: Jun. 7, 2018. [Online]. Available:
http://projects.csail.mit.edu/stata/
[42] M. Fallon, H. Johannsson, M. Kae ss, and J. J. Leonard, “The MIT stata
center dataset,” Int. J. Robot. Res. , vol. 32, no. 14, pp. 1695–1699, 2013.
Li-Hsin Chen received the B.S. and M.S. degrees
from the Department of Ae ronautics and Astro-
nautics (DAA), National Cheng Kung University(NCKU), Tainan, Taiwan, in 2016 and 2018, respec-
tively, where he is currently pursuing the Ph.D.
degree.
His research interests include system dynamics
analysis, optimization, and SLAM technology.
Chao-Chung Peng was born in Kaohsiung, Taiwan,
in 1980. He received the B.S. degree from theDepartment of Aeronautic s and Astronautics (DAA),
National Cheng Kung Univ ersity (NCKU), Tainan,
Taiwan, in 2003, and the Ph.D. degree in 2009.
From 2008 to 2009, he was a Research Assistant
with the Department of Engineering, Leicester Uni-versity, U.K. In 2009, he was awarded a member-
ship from the Phi Tau Phi Scholastic Honor Society.
From 2010 to 2012, he was a Post-Doctoral Fellowwith the Department of Mechanical Engineering,
NCKU. In 2012, he moved to ADLINK Technology and served as a Senior
Engineer in the Embedded System Development Section, Measurement andAutomation Department. From 2014 to 2016, he was with the Automationand Instrumentation System Development Section, Iron and Steel R&D
Department, China Steel Corpora tion (CSC). Since 2016, he has been an
Assistant Professor with the Department of Aeronautics and Astronautics,NCKU. His research interests include high performance motion control and
applications, unmanned vehicle desi gn and advanced ﬂight control system
development, autonomous robotics and intelligence SLAM technology, andsystem modeling and diagnosis.
Authorized licensed use limited to: Univ of  Calif San Diego. Downloaded on July 15,2022 at 16:19:25 UTC from IEEE Xplore.  Restrictions apply. 